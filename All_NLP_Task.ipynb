{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK9GjymDJL1H",
        "outputId": "36cd9555-d204-4504-b457-4c8f0f882b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk as nl\n",
        "nl.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentance tokenization\n",
        "import nltk as nl\n",
        "#import random\n",
        "example_text = 'a real movie , about real people. Movie gives us a rare glimpse into a culture most of us don\\'t know .'\n",
        "print(nl.tokenize.sent_tokenize(example_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bV_Re4QkJVFE",
        "outputId": "27aff713-17c4-428b-bbbb-e2f63edb096a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a real movie , about real people.', \"Movie gives us a rare glimpse into a culture most of us don't know .\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization\n",
        "example = 'Movie gives us a rare glimpse into a culture most of us don\\'t know .'\n",
        "\n",
        "tokenized_words = nl.tokenize.word_tokenize(example)\n",
        "print(tokenized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCINDFhgJdfi",
        "outputId": "d677acc8-2381-4c74-90a6-b6bd614c04e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Movie', 'gives', 'us', 'a', 'rare', 'glimpse', 'into', 'a', 'culture', 'most', 'of', 'us', 'do', \"n't\", 'know', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pos tagging\n",
        "nl.download('averaged_perceptron_tagger')\n",
        "tagged_words = nl.tag.pos_tag(tokenized_words)\n",
        "print(tagged_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXvL_DcHJrHB",
        "outputId": "f74aba35-ca6a-4206-8e44-bba6ebb6c7b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Movie', 'NNP'), ('gives', 'VBZ'), ('us', 'PRP'), ('a', 'DT'), ('rare', 'JJ'), ('glimpse', 'NN'), ('into', 'IN'), ('a', 'DT'), ('culture', 'NN'), ('most', 'JJS'), ('of', 'IN'), ('us', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# removal of stop words\n",
        "nl.download('stopwords')\n",
        "\n",
        "stop_words = set(nl.corpus.stopwords.words('english'))\n",
        "\n",
        "word_tokens = nl.tokenize.word_tokenize(example)\n",
        "\n",
        "filtered_words = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_words.append(w)\n",
        "print('stop words provided by NLTK package----\\n')\n",
        "print(stop_words)\n",
        "print('\\n filtered data after stop word removal \\n')\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx5hhJUqKMiL",
        "outputId": "58e84af1-9295-43cf-af7e-928808a8209e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stop words provided by NLTK package----\n",
            "\n",
            "{'any', 'my', 'but', 'at', 'theirs', 'd', 'm', \"she's\", 'shan', \"you're\", 'each', 'about', 'she', 'which', 'where', 'ain', \"you'll\", 'how', 'don', 'couldn', 's', \"that'll\", 'll', 'won', 'while', 'its', 'is', 'were', 'be', 'does', 'when', \"wouldn't\", \"needn't\", \"mustn't\", 'are', \"aren't\", 'do', 'there', 'hadn', 'their', 'further', 'through', 'and', 'isn', 'why', 'shouldn', 'what', 'him', 'needn', 'below', 'off', 'over', 'am', \"doesn't\", \"isn't\", 'ma', 'the', 'his', 'both', 'down', 'out', 'too', 'her', 'those', 'above', 'hasn', \"shouldn't\", 'they', 'aren', 'before', 'most', 'after', 'weren', 'by', 'with', 'wouldn', \"wasn't\", 'yourselves', 'an', 'once', 'now', 'between', 'in', 'into', 'me', 'just', 'having', 'you', \"hasn't\", 'this', 'as', 'doing', 'because', 'ourselves', 'y', 'doesn', 'hers', \"couldn't\", 'own', 'these', \"don't\", 'on', 'until', 'been', 'wasn', \"weren't\", 'he', 'that', 'very', 'mustn', 'myself', 'up', 'them', 'only', 'i', 'our', \"it's\", 're', 'all', 'can', 'for', 'to', 'have', \"you've\", \"you'd\", 'other', 'from', 'being', 'whom', 'it', 'against', 'under', \"hadn't\", 'did', \"haven't\", \"didn't\", 'haven', 'again', 'herself', 'should', 've', \"should've\", \"won't\", 'here', 'such', 'or', 'itself', 'no', \"mightn't\", 't', 'yours', 'few', 'then', 'mightn', 'has', \"shan't\", 'didn', 'during', 'himself', 'your', 'who', 'a', 'themselves', 'more', 'will', 'than', 'if', 'not', 'same', 'ours', 'of', 'had', 'we', 'o', 'nor', 'so', 'some', 'yourself', 'was'}\n",
            "\n",
            " filtered data after stop word removal \n",
            "\n",
            "['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#punctuation mark removal\n",
        "p = [',','?','.']\n",
        "word_tokens = filtered_words\n",
        "\n",
        "\n",
        "filtered2 = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in p:\n",
        "        filtered2.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK8E2BZkKiQZ",
        "outputId": "19be98c2-13ef-4cd6-f38f-8e6ea067bdc4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know', '.']\n",
            "['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "\n",
        "ps = nl.stem.PorterStemmer()\n",
        "set1 =['Movie', 'gives', 'us', 'rare', 'glimpse', 'culture', 'us', \"n't\", 'know']\n",
        "\n",
        "for w in set1:\n",
        "    print(ps.stem(w))\n",
        "\n",
        "set2 =[ \"python\",\"pythoner\",\"pythoning\",\"pythoned\"]\n",
        "set2 =[ \"banker\",\"banking\",\"bank\"]\n",
        "\n",
        "print('-------------------')\n",
        "for w in set2:\n",
        "    print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRbKyhKQKu2i",
        "outputId": "191a6a28-46d8-42ae-9e33-a1cbf3589042"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movi\n",
            "give\n",
            "us\n",
            "rare\n",
            "glimps\n",
            "cultur\n",
            "us\n",
            "n't\n",
            "know\n",
            "-------------------\n",
            "banker\n",
            "bank\n",
            "bank\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data set loading\n",
        "positive = open('/content/sample_data/rt-polarity-pos.txt', encoding='latin1')\n",
        "negative = open('/content/sample_data/rt-polarity-neg.txt', encoding='latin1')\n",
        "\n",
        "i=0\n",
        "while i<5 :\n",
        "    print(negative.readline())\n",
        "    i+=1\n",
        "\n",
        "#print(positive.readlines())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS7FKYZsK0xK",
        "outputId": "398c2b9f-da01-4512-ce05-f357fe579190"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simplistic , silly and tedious . \n",
            "\n",
            "it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
            "\n",
            "exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
            "\n",
            "[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
            "\n",
            "a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing\n",
        "def remove_stop_words(w_token):\n",
        "    stop_words = set(nl.corpus.stopwords.words('english'))\n",
        "    filtered_words = []\n",
        "    ps = nl.stem.PorterStemmer()\n",
        "    for tmp_word in w_token:\n",
        "        if tmp_word not in stop_words:\n",
        "            filtered_words.append(tmp_word)\n",
        "    return filtered_words\n",
        "\n",
        "def process_sentence(s):\n",
        "    w_token = nl.tokenize.word_tokenize(s)\n",
        "    punctuations = [',','?','.',']','[','}','{','(',')','!','?',':',';','\"','\\'']\n",
        "    t2 = []\n",
        "    for w in w_token:\n",
        "        if w not in punctuations:\n",
        "            t2.append(w)\n",
        "    t3 = remove_stop_words(t2)\n",
        "    return {word: 1 for word in t3}\n",
        "\n",
        "\n",
        "positive_data_array = []\n",
        "\n",
        "# positive review prcocessing\n",
        "for p_review in positive:\n",
        "    positive_data_array.append([process_sentence(p_review),'positive'])\n",
        "\n",
        "\n",
        "\n",
        "negative_data_array = []\n",
        "i= 0\n",
        "\n",
        "#negative review processing\n",
        "for n_review in negative:\n",
        "        processed = [process_sentence(n_review),'negative']\n",
        "        negative_data_array.append(processed)\n",
        "\n",
        "        if(i<5):\n",
        "            print('review before processing->')            # demo purpose code to see the affect\n",
        "            print(n_review)\n",
        "            print('review after processing->')\n",
        "            print(processed)\n",
        "            print('-------------------------')\n",
        "            i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG7_NkL9K7OF",
        "outputId": "bcc59bc0-5c30-45a4-80d5-6a78cf1bb4a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "review before processing->\n",
            "the story is also as unoriginal as they come , already having been recycled more times than i'd care to count . \n",
            "\n",
            "review after processing->\n",
            "[{'story': 1, 'also': 1, 'unoriginal': 1, 'come': 1, 'already': 1, 'recycled': 1, 'times': 1, \"'d\": 1, 'care': 1, 'count': 1}, 'negative']\n",
            "-------------------------\n",
            "review before processing->\n",
            "about the only thing to give the movie points for is bravado -- to take an entirely stale concept and push it through the audience's meat grinder one more time . \n",
            "\n",
            "review after processing->\n",
            "[{'thing': 1, 'give': 1, 'movie': 1, 'points': 1, 'bravado': 1, '--': 1, 'take': 1, 'entirely': 1, 'stale': 1, 'concept': 1, 'push': 1, 'audience': 1, \"'s\": 1, 'meat': 1, 'grinder': 1, 'one': 1, 'time': 1}, 'negative']\n",
            "-------------------------\n",
            "review before processing->\n",
            "not so much farcical as sour . \n",
            "\n",
            "review after processing->\n",
            "[{'much': 1, 'farcical': 1, 'sour': 1}, 'negative']\n",
            "-------------------------\n",
            "review before processing->\n",
            "unfortunately the story and the actors are served with a hack script . \n",
            "\n",
            "review after processing->\n",
            "[{'unfortunately': 1, 'story': 1, 'actors': 1, 'served': 1, 'hack': 1, 'script': 1}, 'negative']\n",
            "-------------------------\n",
            "review before processing->\n",
            "all the more disquieting for its relatively gore-free allusions to the serial murders , but it falls down in its attempts to humanize its subject . \n",
            "\n",
            "review after processing->\n",
            "[{'disquieting': 1, 'relatively': 1, 'gore-free': 1, 'allusions': 1, 'serial': 1, 'murders': 1, 'falls': 1, 'attempts': 1, 'humanize': 1, 'subject': 1}, 'negative']\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#partition into training and test set\n",
        "\n",
        "\n",
        "# shuffling\n",
        "import random\n",
        "random.shuffle(positive_data_array)\n",
        "random.shuffle(negative_data_array)\n",
        "\n",
        "#partitioning\n",
        "training_set = positive_data_array[:3000]+negative_data_array[:3000]\n",
        "test_set =positive_data_array[3000:]+negative_data_array[3000:]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_1pCuYGzLp-M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build classifier and test\n",
        "classifier = nl.NaiveBayesClassifier.train(training_set)\n",
        "\n",
        "\n",
        "print('Accuracy: ',nl.classify.util.accuracy(classifier,test_set))\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "TP = 0\n",
        "TN = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "for itr in range(len(test_set)):\n",
        "  out=classifier.classify(test_set[itr][0])\n",
        "\n",
        "  if out=='positive' and test_set[itr][1]=='positive':\n",
        "        TP = TP + 1\n",
        "  if out=='negative' and test_set[itr][1]=='negative':\n",
        "        TN = TN + 1\n",
        "  if out=='positive' and test_set[itr][1]=='negative':\n",
        "        FP = FP + 1\n",
        "  if out=='negative' and test_set[itr][1]=='positive':\n",
        "        FN = FN + 1\n",
        "\n",
        "print('True Positive:',TP,'\\t False Positive:',FP)\n",
        "print('False Negative:',FN,'\\t True Negative:',TN)\n",
        "\n",
        "print('\\n')\n",
        "print('Confusion Matrix:\\n')\n",
        "print('_________________________________________________')\n",
        "print('\\t\\t\\tActual Output\\n')\n",
        "print('_________________________________________________')\n",
        "print('\\t\\t',TP,'\\t\\t',FP)\n",
        "print('Test Output -------------------------------------')\n",
        "print('\\t\\t',FN,'\\t\\t',TN)\n",
        "print('_________________________________________________')\n",
        "print('\\n')\n",
        "print('Precision:[TP/(TP+FP)]\\t',TP/(TP+FP))\n",
        "print('Recall:[TP/(TP+FN)]\\t',TP/(TP+FN))\n",
        "print('Accuaracy:[(TP+TN)/(TP+TN+FP+FN)]\\t',(TP+TN)/(TP+TN+FP+FN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8Ky_50WL0OQ",
        "outputId": "8cad7b36-0084-4fab-a511-66c737bd2167"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.7582134421301266\n",
            "\n",
            "\n",
            "True Positive: 1796 \t False Positive: 591\n",
            "False Negative: 535 \t True Negative: 1735\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "\n",
            "_________________________________________________\n",
            "\t\t\tActual Output\n",
            "\n",
            "_________________________________________________\n",
            "\t\t 1796 \t\t 591\n",
            "Test Output -------------------------------------\n",
            "\t\t 535 \t\t 1735\n",
            "_________________________________________________\n",
            "\n",
            "\n",
            "Precision:[TP/(TP+FP)]\t 0.7524088814411395\n",
            "Recall:[TP/(TP+FN)]\t 0.7704847704847705\n",
            "Accuaracy:[(TP+TN)/(TP+TN+FP+FN)]\t 0.7582134421301266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classifiy new test instance\n",
        "\n",
        "print(classifier.classify(process_sentence('very bad movie. I have wasted my money.')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xwWHmr8MF1C",
        "outputId": "8e2e2cab-f617-450f-d6c0-c0a859f93477"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classifier.classify(process_sentence('One of the best movie, I have ever seen.')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzci-mGxMXvO",
        "outputId": "f0254ab0-b815-4c34-ec2c-a724786d78a6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Review taken from the PPT & pre-processed in above code cells\n",
        "\n",
        "print(classifier.classify(process_sentence('a real movie , about real people , that gives us a rare glimpse into a culture most of us don\\'t know .')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsbegrOMMcbn",
        "outputId": "ee5cf372-c4fd-42ae-c2ac-ae07bbee21b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XKeEOqa7Mgxu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}